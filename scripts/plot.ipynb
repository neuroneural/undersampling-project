{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot() missing 1 required positional argument: 'plot_no'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 277\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    274\u001b[0m                     \u001b[38;5;66;03m#print(f'error poltting {signal_dataset} {noise_dataset} {kernel_type} sklearn {sklearn}')\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msklearn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: plot() missing 1 required positional argument: 'plot_no'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "signal_dataset = 'oulu'\n",
    "noise_dataset = 'cobre'\n",
    "\n",
    "plot_all = True\n",
    "\n",
    "kernel_type = 'rbf'\n",
    "sklearn = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot(noise_dataset, signal_dataset, sklearn, kernel_type, plot_no):\n",
    "\n",
    "    # Define the directory for the current undersampling rate\n",
    "    pkl_dir = f'/data/users2/jwardell1/undersampling-project/{signal_dataset.upper()}/pkl-files/{noise_dataset.upper()}'\n",
    "    #print(pkl_dir)\n",
    "\n",
    "    if sklearn:\n",
    "        joined_files = os.path.join(f'{pkl_dir}/SVM/sklearn', f'*{kernel_type}*.pkl')\n",
    "    else:\n",
    "        joined_files = os.path.join(f'{pkl_dir}/SVM', f'*{kernel_type}*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    #print(joined_list)\n",
    "    thundersvm_df = pd.concat((pd.read_pickle(file).assign(classifier='SVM') for file in joined_list), ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Read and concatenate sr1 data\n",
    "    joined_files = os.path.join(pkl_dir, 'sr1_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    sr1 = pd.concat((pd.read_pickle(file).assign(sampling_rate='sr1') for file in joined_list), ignore_index=True)\n",
    "\n",
    "    # Read and concatenate sr2 data\n",
    "    joined_files = os.path.join(pkl_dir, 'sr2_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    sr2 = pd.concat((pd.read_pickle(file).assign(sampling_rate='sr2') for file in joined_list), ignore_index=True)\n",
    "\n",
    "    # Read and concatenate concat data\n",
    "    joined_files = os.path.join(pkl_dir, 'concat_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    concat = pd.concat((pd.read_pickle(file).assign(sampling_rate='concat') for file in joined_list), ignore_index=True)\n",
    "\n",
    "    # Read and concatenate add data\n",
    "    joined_files = os.path.join(pkl_dir, 'add_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    add = pd.concat((pd.read_pickle(file).assign(sampling_rate='add') for file in joined_list), ignore_index=True)\n",
    "\n",
    "\n",
    "    # Read and concatenate add data\n",
    "    joined_files = os.path.join(pkl_dir, 'interleaved_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    interleaved = pd.concat((pd.read_pickle(file).assign(sampling_rate='interleaved') for file in joined_list), ignore_index=True)\n",
    "\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    all_data = pd.concat([sr1, sr2, concat, add, interleaved], ignore_index=True)\n",
    "\n",
    "    # Function to split each row into multiple rows based on cross-validation index\n",
    "    def split_row(row):\n",
    "        new_rows = []\n",
    "        for cv_ix, auc in enumerate(row['test_scores']):\n",
    "            new_row = row.copy()\n",
    "            new_row['auc'] = auc\n",
    "            new_row['cv_ix'] = cv_ix\n",
    "            new_rows.append(new_row)\n",
    "        return pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "    all_data = all_data.dropna(subset=['test_scores'])\n",
    "\n",
    "    # Apply the split_row function to each row of the dataframe and concatenate the results\n",
    "    result_df = pd.concat(all_data.apply(split_row, axis=1).tolist(), ignore_index=True)\n",
    "\n",
    "    # Drop the original 'test_scores' column as it's no longer needed\n",
    "    result_df.drop('test_scores', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Load thundersvm_df_linear and rename 'roc' column to 'auc'\n",
    "    thundersvm_df.rename(columns={'roc': 'auc'}, inplace=True)\n",
    "    thundersvm_df.rename(columns={'fold': 'cv_ix'}, inplace=True)\n",
    "    thundersvm_df.rename(columns={'noise_ix': 'noise_no'}, inplace=True)\n",
    "    #thundersvm_df_rbf.rename(columns={'fbirn_sub': 'graph_no'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Filter and clean up thundersvm_df_linear\n",
    "    thundersvm_df_linear = thundersvm_df[['snr', 'cv_ix', 'auc', 'sampling_rate', 'classifier']]#'noise_no']#'graph_no', 'classifier']]\n",
    "    thundersvm_df_linear = thundersvm_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result_df = result_df[['snr', 'cv_ix', 'auc', 'sampling_rate', 'noise_no', 'classifier']]# 'graph_no', 'classifier']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Merge with thundersvm_df_linear\n",
    "    result_df = pd.concat([result_df, thundersvm_df_linear], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('add', 'Add')\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('concat', 'Concat')\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('interleaved', 'Interleaved')\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr1', 'TR100')\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr2', 'TR2150')\n",
    "\n",
    "\n",
    "    result_df = result_df[result_df['snr'] <= 3]\n",
    "\n",
    "\n",
    "    ###########################################################################################################################\n",
    "\n",
    "    grouped_classifiers = result_df.groupby('classifier')\n",
    "\n",
    "    classifier_dfs = {}\n",
    "    for classifier, group_df in grouped_classifiers:\n",
    "        classifier_dfs[classifier] = group_df.copy()\n",
    "\n",
    "    mlp_df = classifier_dfs.get('Multilayer Perceptron', pd.DataFrame())\n",
    "    lr_df = classifier_dfs.get('Logistic Regression', pd.DataFrame())\n",
    "    svm_df = classifier_dfs.get('SVM', pd.DataFrame())\n",
    "    nb_df = classifier_dfs.get('Naive Bayes', pd.DataFrame())\n",
    "\n",
    "    dfs = [mlp_df, lr_df, svm_df, nb_df]\n",
    "    classifiers = ['Multilayer Perceptron', 'Logistic Regression', 'SVM', 'Naive Bayes']\n",
    "    sampling_rates = ['TR100', 'TR2150', 'Add', 'Concat', 'Interleaved']\n",
    "    hue_order = sampling_rates\n",
    "    palette = {item: plt.cm.tab20(i) for i, item in enumerate(sampling_rates)}\n",
    "\n",
    "    # Combine all dataframes into one dataframe for plotting\n",
    "    combined_df = pd.concat(dfs)\n",
    "    snr_levels = [1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5]\n",
    "\n",
    "\n",
    "\n",
    "    # Define your variables\n",
    "    x = \"snr\"\n",
    "    y = \"auc\"\n",
    "    hue = \"sampling_rate\"\n",
    "    order = snr_levels\n",
    "\n",
    "    # Create the figure and the axes\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20, 12), gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "    # Collect y-limits for determining the common range\n",
    "    all_y_vals = []\n",
    "\n",
    "    # Plot the first row with sampling_rate as hue\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        # Filter the data for the current classifier\n",
    "        data_filtered = combined_df[combined_df['classifier'] == classifier]\n",
    "\n",
    "        # Lineplot with error bands\n",
    "        summary_df = data_filtered.groupby([x, hue]).agg(\n",
    "            mean_auc=(y, 'mean'),\n",
    "            std_auc=(y, 'std'),\n",
    "            count=(y, 'count')\n",
    "        ).reset_index()\n",
    "        summary_df['se_auc'] = summary_df['std_auc'] / np.sqrt(summary_df['count'])\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=summary_df, x=x, y='mean_auc', hue=hue, hue_order=hue_order,\n",
    "            palette=palette, marker='o', ax=axes[0, i]\n",
    "        )\n",
    "        \n",
    "        # Add error bands manually\n",
    "        for key, grp in summary_df.groupby(hue):\n",
    "            color = palette[key]\n",
    "            axes[0, i].fill_between(grp[x], grp['mean_auc'] - grp['se_auc'], grp['mean_auc'] + grp['se_auc'], color=color, alpha=0.2)\n",
    "        \n",
    "        if classifier == 'SVM':\n",
    "            axes[0, i].set_title(f'{classifier} ({kernel_type})')\n",
    "        else:\n",
    "            axes[0, i].set_title(f'{classifier}')\n",
    "        axes[0, i].set_ylim(0.5, 1)  # Setting y-limit\n",
    "        axes[0, i].set_xlabel('SNR')\n",
    "        axes[0, i].set_ylabel('AUC')\n",
    "\n",
    "\n",
    "    # Adjust hue_order and palette for the second row\n",
    "    hue_order = classifiers\n",
    "    classifier_palette = sns.color_palette(\"Set1\", len(classifiers))\n",
    "    palette = {item: classifier_palette[i] for i, item in enumerate(classifiers)}\n",
    "\n",
    "    # Plot the second row with classifier as hue\n",
    "    for i, sampling_rate in enumerate(sampling_rates):\n",
    "        # Filter the data for the current sampling rate\n",
    "        data_filtered = combined_df[combined_df['sampling_rate'] == sampling_rate]\n",
    "\n",
    "        # Lineplot with error bands\n",
    "        summary_df = data_filtered.groupby([x, 'classifier']).agg(\n",
    "            mean_auc=(y, 'mean'),\n",
    "            std_auc=(y, 'std'),\n",
    "            count=(y, 'count')\n",
    "        ).reset_index()\n",
    "        summary_df['se_auc'] = summary_df['std_auc'] / np.sqrt(summary_df['count'])\n",
    "        \n",
    "        sns.lineplot(\n",
    "            data=summary_df, x=x, y='mean_auc', hue='classifier', hue_order=hue_order,\n",
    "            palette=palette, marker='o', ax=axes[1, i]\n",
    "        )\n",
    "        \n",
    "        # Add error bands manually\n",
    "        for key, grp in summary_df.groupby('classifier'):\n",
    "            color = palette[key]\n",
    "            axes[1, i].fill_between(grp[x], grp['mean_auc'] - grp['se_auc'], grp['mean_auc'] + grp['se_auc'], color=color, alpha=0.2)\n",
    "        \n",
    "        # Collect y-values\n",
    "        all_y_vals.extend(data_filtered[y].values)\n",
    "        \n",
    "        axes[1, i].set_title(f'{sampling_rate}')\n",
    "        axes[1, i].set_ylim(0.5, 1)  # Setting y-limit\n",
    "        axes[1, i].set_xlabel('SNR')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel('AUC')\n",
    "        else:\n",
    "            axes[1, i].set_ylabel('')\n",
    "\n",
    "\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Add a super title\n",
    "    #fig.suptitle(f\"Finnish Dataset AUC vs SNR using FBIRN subject {subID} Noise\", fontsize=16, y=1.10)\n",
    "    fig.suptitle(f\"PLOT {plot_no}: {signal_dataset.upper()} Dataset AUC vs SNR using {noise_dataset.upper()} Noise\", fontsize=16, y=1.10)\n",
    "\n",
    "    # Add grid\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.grid(True, which='both', axis='both', linestyle='--')\n",
    "\n",
    "    # Add legends\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(sampling_rates), title='Sampling Rate', bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    handles, labels = axes[1, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=len(classifiers), title='Classifier', bbox_to_anchor=(0.5, -0.05))\n",
    "    plt.savefig(f'../results/{signal_dataset}_{noise_dataset}_{kernel_type}.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if plot_all:\n",
    "    signal_datasets = ['oulu', 'hcp']\n",
    "    noise_datasets = ['fbirn', 'cobre', 'var']\n",
    "    kernels = ['rbf']#, 'linear']\n",
    "    plot_no = 0\n",
    "\n",
    "    for signal_dataset in signal_datasets:\n",
    "        for noise_dataset in noise_datasets:\n",
    "            for kernel_type in kernels:\n",
    "                try:\n",
    "                    plot(noise_dataset, signal_dataset, sklearn, kernel_type, plot_no)\n",
    "                    plot_no += 1\n",
    "                except ValueError as e:\n",
    "                    pass\n",
    "                    #print(f'error poltting {signal_dataset} {noise_dataset} {kernel_type} sklearn {sklearn}')\n",
    "\n",
    "else:\n",
    "    plot(noise_dataset, signal_dataset, sklearn, kernel_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "add_stats_annotations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
