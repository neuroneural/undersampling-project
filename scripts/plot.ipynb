{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/users2/jwardell1/undersampling-project/OULU/pkl-files/FBIRN/11-11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "signal_dataset = 'oulu'\n",
    "noise_dataset = 'fbirn'\n",
    "\n",
    "plot_all = False\n",
    "\n",
    "kernel_type = 'rbf'\n",
    "\n",
    "optuna = False\n",
    "\n",
    "date = '11-11'\n",
    "\n",
    "\n",
    "def plot(noise_dataset, signal_dataset, kernel_type, plot_no, optuna):\n",
    "\n",
    "    # Define the directory for the current undersampling rate\n",
    "    pkl_dir = f'/data/users2/jwardell1/undersampling-project/{signal_dataset.upper()}/pkl-files/{noise_dataset.upper()}/optuna/{date}' if optuna \\\n",
    "        else f'/data/users2/jwardell1/undersampling-project/{signal_dataset.upper()}/pkl-files/{noise_dataset.upper()}/{date}'\n",
    "    print(pkl_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Read and concatenate sr1 data\n",
    "    joined_files = os.path.join(pkl_dir, 'sr1_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    sr1 = pd.concat(\n",
    "        (pd.read_pickle(file).assign(\n",
    "            sampling_rate=file.split('_')[0],\n",
    "            subject_id=file.split('_')[5].split('.')[0]\n",
    "        ) for file in joined_list),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Read and concatenate sr2 data\n",
    "    joined_files = os.path.join(pkl_dir, 'sr2_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    sr2 = pd.concat(\n",
    "        (pd.read_pickle(file).assign(\n",
    "            sampling_rate='sr2',\n",
    "            subject_id=file.split('_')[5].split('.')[0]\n",
    "        ) for file in joined_list),\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Read and concatenate concat data\n",
    "    joined_files = os.path.join(pkl_dir, 'concat_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    concat = pd.concat(\n",
    "        (pd.read_pickle(file).assign(\n",
    "            sampling_rate='concat',\n",
    "            subject_id=file.split('_')[5].split('.')[0]\n",
    "        ) for file in joined_list),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Read and concatenate add data\n",
    "    joined_files = os.path.join(pkl_dir, 'add_*.pkl')\n",
    "    joined_list = glob.glob(joined_files)\n",
    "    add = pd.concat(\n",
    "        (pd.read_pickle(file).assign(\n",
    "            sampling_rate='add',\n",
    "            subject_id=file.split('_')[5].split('.')[0]\n",
    "        ) for file in joined_list),\n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    if optuna:\n",
    "        result_df = pd.concat([sr1, sr2, concat, add], ignore_index=True)\n",
    "        result_df = result_df.rename(columns={'roc': 'auc'})\n",
    "\n",
    "        result_df['classifier'] = result_df['classifier'].replace('lr', 'Logistic Regression')\n",
    "        result_df['classifier'] = result_df['classifier'].replace('mlp', 'Multilayer Perceptron')\n",
    "        result_df['classifier'] = result_df['classifier'].replace('svm', 'SVM')\n",
    "        result_df['classifier'] = result_df['classifier'].replace('nb', 'Naive Bayes')\n",
    "    else:\n",
    "        all_data = pd.concat([sr1, sr2, concat, add], ignore_index=True)\n",
    "        \n",
    "    \n",
    "\n",
    "    if not optuna:\n",
    "        # Function to split each row into multiple rows based on cross-validation index\n",
    "        def split_row(row):\n",
    "            new_rows = []\n",
    "            for cv_ix, auc in enumerate(row['test_scores']):\n",
    "                new_row = row.copy()\n",
    "                new_row['auc'] = auc\n",
    "                new_row['cv_ix'] = cv_ix\n",
    "                new_rows.append(new_row)\n",
    "            return pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "        all_data = all_data.dropna(subset=['test_scores'])\n",
    "\n",
    "        # Apply the split_row function to each row of the dataframe and concatenate the results\n",
    "        result_df = pd.concat(all_data.apply(split_row, axis=1).tolist(), ignore_index=True)\n",
    "\n",
    "        # Drop the original 'test_scores' column as it's no longer needed\n",
    "        result_df.drop('test_scores', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        result_df = result_df[['snr', 'cv_ix', 'auc', 'sampling_rate', 'noise_no', 'classifier']]# 'graph_no', 'classifier']]\n",
    "    \n",
    "\n",
    "\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('add', 'Add')\n",
    "    result_df['sampling_rate'] = result_df['sampling_rate'].replace('concat', 'Concat')\n",
    "    if signal_dataset == 'simulation':\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr1', 'TR2200')\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr2', 'TR2600')\n",
    "\n",
    "    if signal_dataset == 'oulu':\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr1', 'TR100')\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr2', 'TR2150')\n",
    "\n",
    "    if signal_dataset == 'hcp':\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr1', 'SR1')\n",
    "        result_df['sampling_rate'] = result_df['sampling_rate'].replace('sr2', 'SR2')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    result_df = result_df[result_df['snr'] <= 3]\n",
    "\n",
    "    current_time = str(int(time.time()))\n",
    "    result_df.to_csv(f'oulu-fbirn-result_df_{current_time}.csv')\n",
    "    return\n",
    "\n",
    "    '''\n",
    "    \n",
    "    ###########################################################################################################################\n",
    "\n",
    "    grouped_classifiers = result_df.groupby('classifier')\n",
    "\n",
    "    classifier_dfs = {}\n",
    "    for classifier, group_df in grouped_classifiers:\n",
    "        classifier_dfs[classifier] = group_df.copy()\n",
    "\n",
    "    mlp_df = classifier_dfs.get('Multilayer Perceptron', pd.DataFrame())\n",
    "    lr_df = classifier_dfs.get('Logistic Regression', pd.DataFrame())\n",
    "    svm_df = classifier_dfs.get('SVM', pd.DataFrame())\n",
    "    nb_df = classifier_dfs.get('Naive Bayes', pd.DataFrame())\n",
    "\n",
    "    if signal_dataset == 'simulation':\n",
    "        tr1 = 'TR2200'\n",
    "        tr2 = 'TR2600'\n",
    "\n",
    "    if signal_dataset == 'oulu':        \n",
    "        tr1 = 'TR100'\n",
    "        tr2 = 'TR2150'\n",
    "\n",
    "    if signal_dataset == 'hcp':\n",
    "        tr1 = 'SR1'\n",
    "        tr2 = 'SR2'\n",
    "\n",
    "    dfs = [mlp_df, lr_df, svm_df, nb_df]\n",
    "    classifiers = ['Multilayer Perceptron', 'Logistic Regression', 'SVM', 'Naive Bayes']\n",
    "    \n",
    "    sampling_rates = [tr1, tr2, 'Add', 'Concat']\n",
    "    hue_order = sampling_rates\n",
    "    palette = {item: plt.cm.tab20(i) for i, item in enumerate(sampling_rates)}\n",
    "\n",
    "    # Combine all dataframes into one dataframe for plotting\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    snr_levels = [1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5]\n",
    "\n",
    "    time = str(int(time.time()))\n",
    "    combined_df.to_csv(f'oulu-fbirn-combined_df_{time}.csv')\n",
    "\n",
    "    # Define your variables\n",
    "    x = \"snr\"\n",
    "    y = \"auc\"\n",
    "    hue = \"sampling_rate\"\n",
    "    order = snr_levels\n",
    "\n",
    "    # Create the figure and the axes\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 12), gridspec_kw={'height_ratios': [1, 1]})\n",
    "\n",
    "    # Collect y-limits for determining the common range\n",
    "    all_y_vals = []\n",
    "\n",
    "    # Plot the first row with sampling_rate as hue\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        # Filter the data for the current classifier\n",
    "        data_filtered = combined_df[combined_df['classifier'] == classifier]\n",
    "\n",
    "        # Lineplot with error bands\n",
    "        #summary_df = data_filtered.groupby([x, hue]).agg(\n",
    "        #    mean_auc=(y, 'mean'),\n",
    "        #    std_auc=(y, 'std'),\n",
    "        #    count=(y, 'count')\n",
    "        #).reset_index()\n",
    "        #summary_df['se_auc'] = summary_df['std_auc'] / np.sqrt(summary_df['count'])\n",
    "        \n",
    "        sns.lineplot(\n",
    "            #data=summary_df, x=x, y='mean_auc', hue=hue, hue_order=hue_order,\n",
    "            data=data_filtered, x=x, y='auc', hue=hue, hue_order=hue_order,\n",
    "            palette=palette, marker='o', ax=axes[0, i], errorbar='sd'\n",
    "        )\n",
    "        \n",
    "        # Add error bands manually\n",
    "        #for key, grp in summary_df.groupby(hue):\n",
    "        #    color = palette[key]\n",
    "        #    axes[0, i].fill_between(grp[x], grp['mean_auc'] - grp['se_auc'], grp['mean_auc'] + grp['se_auc'], color=color, alpha=0.2)\n",
    "        \n",
    "        if classifier == 'SVM':\n",
    "            axes[0, i].set_title(f'{classifier} ({kernel_type})')\n",
    "        else:\n",
    "            axes[0, i].set_title(f'{classifier}')\n",
    "        axes[0, i].set_ylim(0.5, 1)  # Setting y-limit\n",
    "        axes[0, i].set_xlabel('SNR')\n",
    "        axes[0, i].set_ylabel('AUC')\n",
    "\n",
    "\n",
    "    # Adjust hue_order and palette for the second row\n",
    "    hue_order = classifiers\n",
    "    classifier_palette = sns.color_palette(\"Set1\", len(classifiers))\n",
    "    palette = {item: classifier_palette[i] for i, item in enumerate(classifiers)}\n",
    "\n",
    "    # Plot the second row with classifier as hue\n",
    "    for i, sampling_rate in enumerate(sampling_rates):\n",
    "        # Filter the data for the current sampling rate\n",
    "        data_filtered = combined_df[combined_df['sampling_rate'] == sampling_rate]\n",
    "\n",
    "        # Lineplot with error bands\n",
    "        #summary_df = data_filtered.groupby([x, 'classifier']).agg(\n",
    "        #    mean_auc=(y, 'mean'),\n",
    "        #    std_auc=(y, 'std'),\n",
    "        #    count=(y, 'count')\n",
    "        #).reset_index()\n",
    "        #summary_df['se_auc'] = summary_df['std_auc'] / np.sqrt(summary_df['count'])\n",
    "        \n",
    "        sns.lineplot(\n",
    "            #data=summary_df, x=x, y='mean_auc', hue='classifier', hue_order=hue_order,\n",
    "            data=data_filtered, x=x, y='auc', hue='classifier', hue_order=hue_order,\n",
    "            palette=palette, marker='o', ax=axes[1, i], errorbar='sd'\n",
    "        )\n",
    "        \n",
    "        # Add error bands manually\n",
    "        #for key, grp in summary_df.groupby('classifier'):\n",
    "        #    color = palette[key]\n",
    "        #    axes[1, i].fill_between(grp[x], grp['mean_auc'] - grp['se_auc'], grp['mean_auc'] + grp['se_auc'], color=color, alpha=0.2)\n",
    "        \n",
    "        # Collect y-values\n",
    "        all_y_vals.extend(data_filtered[y].values)\n",
    "        \n",
    "        axes[1, i].set_title(f'{sampling_rate}')\n",
    "        axes[1, i].set_ylim(0.5, 1)  # Setting y-limit\n",
    "        axes[1, i].set_xlabel('SNR')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel('AUC')\n",
    "        else:\n",
    "            axes[1, i].set_ylabel('')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Add a super title\n",
    "    #fig.suptitle(f\"Finnish Dataset AUC vs SNR using FBIRN subject {subID} Noise\", fontsize=16, y=1.10)\n",
    "    fig.suptitle(f\"PLOT {plot_no}: {signal_dataset.upper()} Dataset AUC vs SNR using {noise_dataset.upper()} Noise\", fontsize=16, y=1.10)\n",
    "\n",
    "    # Add grid\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            ax.grid(True, which='both', axis='both', linestyle='--')\n",
    "\n",
    "    # Add legends\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', ncol=len(sampling_rates), title='Sampling Rate', bbox_to_anchor=(0.5, 1.05))\n",
    "\n",
    "    handles, labels = axes[1, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=len(classifiers), title='Classifier', bbox_to_anchor=(0.5, -0.05))\n",
    "    #plt.savefig(f'../results/{signal_dataset}_{noise_dataset}_{kernel_type}.png')\n",
    "\n",
    "    plt.show()\n",
    "    return summary_df\n",
    "'''\n",
    "\n",
    "\n",
    "if plot_all:\n",
    "    plot_no = 0\n",
    "    signal_datasets = ['oulu', 'hcp']#, 'simulation']\n",
    "    noise_datasets = ['fbirn', 'FBIRN']#, 'var']\n",
    "    kernels = ['rbf']#, 'linear']\n",
    "\n",
    "    for signal_dataset in signal_datasets:\n",
    "        for noise_dataset in noise_datasets:\n",
    "            for kernel_type in kernels:\n",
    "                try:\n",
    "                    plot(noise_dataset, signal_dataset, kernel_type, plot_no, optuna)\n",
    "                    plot_no += 1\n",
    "                except ValueError as e:\n",
    "                    pass\n",
    "                    #print(f'error poltting {signal_dataset} {noise_dataset} {kernel_type} sklearn {sklearn}')\n",
    "\n",
    "else:\n",
    "    plot(noise_dataset, signal_dataset, kernel_type, 0, optuna)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcombined_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moulu-fbirn.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_df' is not defined"
     ]
    }
   ],
   "source": [
    "combined_df.to_csv('oulu-fbirn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usp_fix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
